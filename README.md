[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)


# Desafio Final ‚Äî Pipeline de Processamento de Resenhas (JetGPT)

README para o projeto que baixa um `.txt` de resenhas, faz parsing, envia cada resenha a um LLM local (LM Studio / Ollama), valida/normaliza com **Pydantic v2**, e gera sa√≠das consolidadas (`processed.json` + `summary.txt`).

---

## üîé Vis√£o geral

Este reposit√≥rio orquestra um pipeline simples e robusto:

1. Baixa a base de dados (`resenhas_app.txt`) para `data/raw/`.
2. L√™ e parseia cada linha no formato `ID$Usu√°rio$Resenha`.
3. Para cada resenha, envia um prompt ao LLM local pedindo **JSON** com `translation_pt` e `sentiment`.
4. Valida e representa dados com **Pydantic v2** (`ReviewRaw`, `ReviewProcessed`).
5. Salva os resultados em `outputs/processed.json` e um resumo em `outputs/summary.txt`.

---

## ‚úÖ Estrutura do projeto (resumida)

```
project_root/
‚îú‚îÄ README.md
‚îú‚îÄ LICENSE
‚îú‚îÄ requirements.txt
‚îú‚îÄ data/
‚îÇ  ‚îî‚îÄ raw/
‚îú‚îÄ outputs/
‚îÇ  ‚îú‚îÄ processed.json
‚îÇ  ‚îî‚îÄ summary.txt
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ config.py                    # leitura de vari√°veis e caminhos
‚îÇ  ‚îú‚îÄ logging_config.py            # configura√ß√£o do logger (fuso BR)
‚îÇ  ‚îú‚îÄ models.py                    # pydantic v2 models
‚îÇ  ‚îú‚îÄ llm_client.py                # wrapper do OpenAI / LLM local
‚îÇ  ‚îú‚îÄ processor.py                 # mapeamento LLM -> ReviewProcessed, an√°lise
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ tools/                       # fun√ß√µes utilit√°rias de dom√≠nio (parsing, prompts)
‚îÇ  ‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îÇ  ‚îú‚îÄ parser.py                 # parsing de linhas txt -> modelos brutos
‚îÇ  ‚îÇ  ‚îú‚îÄ prompt_builder.py         # montar prompt consistente para o LLM
‚îÇ  ‚îÇ  ‚îî‚îÄ text_utils.py             # limpeza de texto, normaliza√ß√£o, detect-language
‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ utils/                       # fun√ß√µes infra/IO/serializa√ß√£o gen√©ricas
‚îÇ     ‚îú‚îÄ __init__.py
‚îÇ     ‚îú‚îÄ io.py                     # salvar/ler JSON/TXT/CSV
‚îÇ     ‚îú‚îÄ file_ops.py               # helpers para paths, cria√ß√£o de pastas, unique names
‚îÇ     ‚îî‚îÄ helpers.py                # pequenas fun√ß√µes compartilhadas (ex: safe_json_load)
‚îú‚îÄ scripts/
‚îÇ  ‚îî‚îÄ run_pipeline.py
‚îî‚îÄ tests/
   ‚îî‚îÄ test_parser.py
```

---

## üõ† Requisitos (m√≠nimos)

* Python 3.10+
* Bibliotecas (instale via `pip`):

```bash
pip install -r requirements.txt
```

Exemplo de `requirements.txt`:

```
pydantic>=2.0
openai
requests
pytz
pandas
```

---

## ‚öôÔ∏è Configura√ß√£o

### 1) Virtualenv (opcional, recomendado)

```bash
python -m venv .venv
source .venv/bin/activate      # macOS / Linux
.venv\Scripts\activate         # Windows
pip install -r requirements.txt
```

### 2) Configurar LLM local

Voc√™ pode configurar a URL/API key diretamente em `src/llm_client.py` ou, prefer√≠vel, via vari√°veis de ambiente que voc√™ leia em `src/config.py`.

Recomenda√ß√µes de vari√°veis (exemplo):

* `LLM_BASE_URL` ‚Äî ex.: `http://127.0.0.1:1234/v1`
* `LLM_API_KEY` ‚Äî ex.: `lm-studio`
* `LLM_MODEL` ‚Äî ex.: `google/gemma-3n-e4b`

Exportando (bash):

```bash
export LLM_BASE_URL="http://127.0.0.1:1234/v1"
export LLM_API_KEY="lm-studio"
export LLM_MODEL="google/gemma-3n-e4b"
```

> Se preferir, apenas edite os valores padr√£o em `src/llm_client.py`.

---

## ‚ñ∂Ô∏è Como rodar (pipeline completo)

1. Configure as vari√°veis (ou edite `src/llm_client.py`).
2. Execute o orquestrador a partir da raiz do projeto:

```bash
python -m scripts.run_pipeline
```

Esse script far√°:

* baixar o `resenhas_app.txt` para `data/raw/` (DocumentLoader),
* ler e parsear o arquivo (reader),
* enviar prompts ao LLM (llm\_client),
* mapear respostas para modelos Pydantic (processor),
* salvar `outputs/processed.json` e `outputs/summary.txt` (utils).

---

## üìÑ Formato dos dados

### Entrada (`.txt`)

Cada linha segue:

```
ID$Usu√°rio$Resenha
```

Ex.: `53409593$Safoan Riyad$)'aimais bien ChatgpT...`

No `reader` usamos `split("$", 2)` para garantir que a resenha possa conter `$`.

### Sa√≠da (`processed.json`)

Lista de objetos com esse modelo (Pydantic v2):

```json
[
  {
    "user": "Safoan Riyad",
    "original": ")'aimais bien ChatgpT. Mais la derni√©re mise 4 jour a tout gach√©. Elle a tout oubli√©.",
    "translation_pt": "Eu gostava mais do ChatGPT. Mas a √∫ltima atualiza√ß√£o arruinou tudo. Ela esqueceu de tudo.",
    "sentiment": "negative",
    "language": "fr",
    "intensity": "Alta",
    "aspects": [
      "atualiza√ß√£o",
      "regress√£o de funcionalidade"
    ],
    "explanation": "O usu√°rio expressa forte insatisfa√ß√£o com a √∫ltima atualiza√ß√£o, que removeu funcionalidades ou conhecimento pr√©vio."
  },
  ...
]
```

### Resumo (`summary.txt`)

Cont√©m:

* contagem por sentimento (positive / negative / neutral)
* string concatenada com todas as resenhas (separador configur√°vel)

---

## üß† Prompt sugerido (muito importante)

Pe√ßa explicitamente **apenas JSON** para facilitar parsing. Exemplo (j√° usado no `scripts/run_pipeline.py`):

```
Por favor retorne um JSON com chaves 'translation_pt' e 'sentiment' para a resenha abaixo.

Resenha: "TEXTO_DA_RESENHA"

Responda somente com JSON. 'sentiment' deve ser 'positive', 'negative' ou 'neutral'.
```

Se o LLM responder fora do formato JSON, o `processor` aplica fallback (sentiment = `neutral`) ‚Äî mas √© melhor garantir respostas JSON.

---

## üß© Observa√ß√µes & boas pr√°ticas

* **Pydantic v2**: usamos `model_dump()` para serializar os modelos.
* **Robustez no parsing**: linhas mal-formatadas s√£o logadas e preenchidas com strings vazias quando poss√≠vel.
* **Logging**: `src/logging_config.py` configura logs no fuso `America/Sao_Paulo`.
* **Batching**: o `LLMClient.batch_process` faz uma chamada por prompt. Se sua API suportar batch nativo, optimize para reduzir lat√™ncia.
* **Rate limits / retry**: dependendo do LLM local e do n√∫mero de resenhas, considere adicionar retries/exponential backoff no client.

---

## üîß Problemas comuns & solu√ß√£o r√°pida

* **Arquivo n√£o baixado**: verifique URL (use `?raw=true` em GitHub) e conectividade. O `DocumentLoader` j√° adiciona `?raw=true` quando detecta `github.com`.
* **Erros de decodifica√ß√£o**: abrimos arquivos com `encoding='utf-8', errors='ignore'` para evitar falhas com caracteres estranhos.
* **LLM retornando texto, n√£o JSON**: ajuste o prompt para refor√ßar "Responda somente com JSON" e reduza temperatura. Use `temperature=0.0`.

---

## Pr√≥ximos passos / melhorias sugeridas

* Implementar retries e controle de taxa (backoff) no `LLMClient`.
* Adicionar testes unit√°rios para `reader.parse_line_to_raw` e para valida√ß√µes Pydantic.
* Exportar outputs adicionais (CSV, an√°lises por idioma, top N palavras).
* Automatizar import para Trello (ex.: criar checklist de progresso com base no `summary.txt`) ‚Äî se quiser, posso gerar esse script.

---

## Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT ‚Äî veja o arquivo [LICENSE](LICENSE) para mais detalhes.
